model:
  d_model: 256
  n_layers: 12
  n_heads: 8
  d_ff: 1024
  max_seq_len: 8192

training:
  batch_size: 128
  gradient_accumulation_steps: 2
  max_epochs: 20
  lr: 0.001
  weight_decay: 0.01
  warmup_steps: 12800
  precision: "bf16-true"
  gradient_clip_val: 1.0
  val_check_interval: 1.0
  num_workers: 16
  val_batch_size: 32

data:
  train_path: output/train.json
  tokenizer_path: tokenizer/tokenizer.json
  preprocessed_dir: output/preprocessed
  train_max_seq_len: 4096
  val_max_seq_len: ${model.max_seq_len}
  max_val_samples: 128
  test_sets:
    iid_combined: output/iid_test.json
    iid_solve: output/iid_test_solve.json
    iid_up: output/iid_test_up.json
    iid_ac: output/iid_test_ac.json
    ood_combined: output/ood_test.json
    ood_solve: output/ood_test_solve.json
    ood_up: output/ood_test_up.json
    ood_ac: output/ood_test_ac.json

wandb:
  project: neural-cdcl
  entity: petr-hyner10
  name: cdcl-d${model.d_model}-L${model.n_layers}-H${model.n_heads}-seq${model.max_seq_len}-bs${training.batch_size}-acc${training.gradient_accumulation_steps}
  tags:
    - cdcl
    - transformer
  table_sample_size: 100

checkpointing:
  dirpath: checkpoints
  monitor: iid/combined/exact_match
  mode: max
  save_top_k: 3
  save_last: true
